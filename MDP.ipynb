{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQieTtO51eY3NnhcLtkVdz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kritibhardwaj23/kriti/blob/main/MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwCJBQrZPB9I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP (Markov Decision Process)\n",
        "num_states = 3\n",
        "num_actions = 2\n",
        "\n",
        "# Define the transition probabilities (P), rewards (R), and initial policy (pi)\n",
        "# These are example values; you should replace them with your specific MDP details.\n",
        "P = np.zeros((num_states, num_actions, num_states))  # Transition probabilities\n",
        "R = np.zeros((num_states, num_actions, num_states))  # Rewards\n",
        "pi = np.zeros((num_states, num_actions))  # Initial policy\n",
        "\n",
        "# Define the discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Policy Iteration Algorithm\n",
        "def policy_evaluation(pi, P, R, gamma):\n",
        "    num_states, num_actions = pi.shape\n",
        "    V = np.zeros(num_states)  # Initialize value function\n",
        "\n",
        " while True:\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            v = V[s]\n",
        "            action = int(pi[s].argmax())\n",
        "            V[s] = sum([P[s, action, s1] * (R[s, action, s1] + gamma * V[s1]) for s1 in range(num_states)])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "        if delta < 1e-6:\n",
        "            break\n",
        "\n",
        "    return V\n",
        "\n",
        "def policy_improvement(pi, P, R, gamma):\n",
        "    num_states, num_actions = pi.shape\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in range(num_states):\n",
        "        old_action = int(pi[s].argmax())\n",
        "\n",
        "        # Compute Q-values for each action\n",
        "        Q = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            Q[a] = sum([P[s, a, s1] * (R[s, a, s1] + gamma * V[s1]) for s1 in range(num_states)])\n",
        "# Update policy to choose the action with the highest Q-value\n",
        "        best_action = np.argmax(Q)\n",
        "        pi[s] = np.eye(num_actions)[best_action]\n",
        "\n",
        "        if best_action != old_action:\n",
        "            policy_stable = False\n",
        "\n",
        "    return pi, policy_stable\n",
        "\n",
        "# Policy Iteration\n",
        "while True:\n",
        "    V = policy_evaluation(pi, P, R, gamma)\n",
        "    pi, policy_stable = policy_improvement(pi, P, R, gamma)\n",
        "\n",
        "    if policy_stable:\n",
        "        break\n",
        "\n",
        "# Print the final policy and value function\n",
        "print(\"Optimal Policy:\")\n",
        "print(pi)\n",
        "print(\"Optimal Value Function:\")\n",
        "print(V)"
      ]
    }
  ]
}